---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

includes:
  talos:
    taskfile: talos
    dir: talos
  bootstrap:
    taskfile: bootstrap
    dir: bootstrap
  flux:
    taskfile: flux
    dir: flux

tasks:
  debug:*:
    desc: Open shell on a node [NODE] [vars NS]
    cmd: |
      kubectl debug node/{{.NODE}} -n {{.NAMESPACE}} -it --image="ghcr.io/nicolaka/netshoot:latest" --profile sysadmin
      kubectl delete pod -n {{.NAMESPACE}} -l app.kubernetes.io/managed-by=kubectl-debug
    vars:
      NODE: "{{index .MATCH 0}}"
      NAMESPACE: '{{.NS | default "default" }}'

  browse-pvc:*:*:
    desc: Browse a PVC [NAMESPACE:CLAIM]
    cmd: kubectl browse-pvc -n {{.NAMESPACE}} -i mirror.gcr.io/alpine:latest {{.CLAIM}}
    vars:
      NAMESPACE: "{{index .MATCH 0}}"
      CLAIM: "{{index .MATCH 1}}"

  debug-pvc:*:*:
    desc: Debug pod with PVC mounted [NAMESPACE:APP] (scales down app first)
    silent: true
    cmd: |
      echo "Scaling down {{.APP}} to release PVC..."
      kubectl scale -n {{.NAMESPACE}} deployment {{.APP}} --replicas=0 2>/dev/null || echo "No deployment to scale"

      echo "Waiting for pod to terminate..."
      kubectl wait --for=delete pod -n {{.NAMESPACE}} -l app.kubernetes.io/name={{.APP}} --timeout=60s 2>/dev/null || true

      echo "Starting debug pod..."
      kubectl run -n {{.NAMESPACE}} debug-{{.APP}}-$(date +%s) \
        --rm -it \
        --image=busybox:latest \
        --overrides='
      {
        "spec": {
          "containers": [{
            "name": "debug",
            "image": "busybox:latest",
            "command": ["sh"],
            "stdin": true,
            "stdinOnce": true,
            "tty": true,
            "volumeMounts": [{
              "name": "data",
              "mountPath": "/data"
            }]
          }],
          "volumes": [{
            "name": "data",
            "persistentVolumeClaim": {
              "claimName": "{{.APP}}"
            }
          }]
        }
      }'

      echo "Scaling {{.APP}} back up..."
      kubectl scale -n {{.NAMESPACE}} deployment {{.APP}} --replicas=1 2>/dev/null || echo "No deployment to scale back"
    vars:
      NAMESPACE: "{{index .MATCH 0}}"
      APP: "{{index .MATCH 1}}"

  cleanup:
    desc: Prune pods in Failed, Pending, or Succeeded state
    cmd: |
      for phase in Failed Pending Succeeded; do
          kubectl delete pods -A --field-selector status.phase="$phase" --ignore-not-found=true
      done

  volsync:*:
    desc: Suspend or resume VolSync ["suspend" | "resume"]
    cmd: |
      flux -n volsync-system {{.STATE}} kustomization volsync
      flux -n volsync-system {{.STATE}} helmrelease volsync
      kubectl -n volsync-system scale deployment volsync --replicas {{if ne .STATE "suspend"}}{{ "1" }}{{else}}{{ "0" }}{{end}}
    vars:
      STATE: "{{index .MATCH 0}}"

  volsync:snapshot:
    desc: Snapshot VolSync PVCs
    cmd: |
      kubectl get replicationsources --no-headers -A | while read -r ns name _; do
          kubectl -n "$ns" patch replicationsources "$name" --type merge -p '{"spec":{"trigger":{"manual":"$(date +%s)"}}}'
      done

  keda:*:
    desc: Suspend or resume Keda ScaledObjects ["suspend" | "resume"]
    cmd: |
      kubectl get scaledobjects --no-headers -A | while read -r ns name _; do
          kubectl -n "$ns" annotate --field-manager flux-client-side-apply --overwrite so "$name" autoscaling.keda.sh/paused{{if ne .STATE "suspend"}}{{ "-" }}{{else}}{{ "=true" }}{{end}}
      done
    vars:
      STATE: "{{index .MATCH 0}}"

  iperf3:
    desc: Run full iperf3 network test matrix between all nodes on 192.168.80.x network
    silent: true
    cmd: |
      echo "=== iperf3 Network Test Matrix (2.5G Network) ==="
      echo ""

      # Dynamically build node to IP mapping from Ansible inventory
      declare -A node_to_ip
      while IFS= read -r node; do
          ip=$(yq eval ".k8s.hosts.$node.internal.ip_addr" {{.INVENTORY}} | cut -d'/' -f1)
          node_to_ip["$node"]="$ip"
      done < <(yq eval '.k8s.hosts | keys | .[]' {{.INVENTORY}})

      # Get all iperf3 pods
      pods=$(kubectl get pods -n network -l app.kubernetes.io/name=iperf3 -o json)
      readarray -t pod_names < <(echo "$pods" | jq -r '.items[].metadata.name')
      readarray -t pod_nodes < <(echo "$pods" | jq -r '.items[].spec.nodeName')

      # Run tests between all pairs on the 192.168.80.x network
      for i in "${!pod_names[@]}"; do
          source_node="${pod_nodes[$i]}"
          source_ip="${node_to_ip[$source_node]}"

          for j in "${!pod_names[@]}"; do
              if [ $i -ne $j ]; then
                  target_node="${pod_nodes[$j]}"
                  target_ip="${node_to_ip[$target_node]}"

                  echo "Testing: $source_node ($source_ip) -> $target_node ($target_ip)"
                  result=$(kubectl exec -n network "${pod_names[$i]}" -c client -- iperf3 -c "$target_ip" -B "$source_ip" -t 3 2>&1 | grep "sender\|receiver" | tail -2)
                  echo "$result"
                  echo ""
              fi
          done
      done

      echo "=== Test Complete ==="
    vars:
      INVENTORY: "{{.TASKFILE_DIR}}/../ansible/inventory.yml"
